# Unsupervised Motion Segmentation for Neuromorphic Aerial Surveillance

This is the official repository for [Unsupervised Motion Segmentation for Neuromorphic Aerial Surveillance
](https://arxiv.org/abs/2405.15209) by [Sami Arja](https://samiarja.com/), Alexandre Marcireau, Saeed Afshar, Bharath Ramesh, Gregory Cohen

![](figures/motion_segmentation_network_EV-Airborne_recording_2023-04-26_15-30-21_cut2.gif)




<table align="center">
  <tr>
    <td align="center" style="border:none;">
      <a href="https://samiarja.github.io/evairborne/" target="_blank">
        <img src="./figures/evairborne.ico" alt="Project Page" width="50" style="padding:10px; background-color: #f5f5f5; border-radius: 10px; box-shadow: 2px 2px 12px #aaa;">
      </a>
    </td>
    <td align="center" style="border:none;">
      <a href="https://arxiv.org/abs/2405.15209" target="_blank">
        <img src="./figures/arxiv.jpeg" alt="Paper" width="50" style="padding:10px; background-color: #f5f5f5; border-radius: 10px; box-shadow: 2px 2px 12px #aaa;">
      </a>
    </td>
    <td align="center" style="border:none;">
      <a href="./figures/SamiArja_Poster_Neuromorphic_Aerial_Surveillance.pdf" target="_blank">
        <img src="./figures/poster_img.png" alt="Poster" width="68" style="padding:10px; background-color: #f5f5f5; border-radius: 10px; box-shadow: 2px 2px 12px #aaa;">
      </a>
    </td>
  </tr>
  <tr>
    <td align="center" style="border:none;">Project Page</td>
    <td align="center" style="border:none;">Paper</td>
    <td align="center" style="border:none;">Poster</td>
  </tr>
</table>

If you use this work in your research, please cite it:

```bibtex
@misc{arja_unsupervised_2024,
	title = {Unsupervised Motion Segmentation for Neuromorphic Aerial Surveillance},
	url = {http://arxiv.org/abs/2405.15209},
	publisher = {arXiv},
	author = {Arja, Sami and Marcireau, Alexandre and Afshar, Saeed and Ramesh, Bharath and Cohen, Gregory},
	month = may,
	year = {2024},
}
```



# Setup

## Requirements

- python: 3.8.x, 3.9.x, 3.10.x

## Tested environments

- Ubuntu 22.04
- Conda 23.1.0
- Python 3.8.18

# Installation

```
conda create --name ev_motion_segmentation python=3.9
conda activate ev_motion_segmentation
python3 -m pip install -e .
pip install torch
pip install tqdm
pip install plotly
pip install scikit-image
pip install loris
pip install PyYAML
pip install opencv-python
conda install -c conda-forge pydensecrf
sudo apt install -y build-essentials
```

# Download dataset
You can download the files from [google drive](https://drive.google.com/drive/folders/1TqXeb1tZFUSCpnRw-I5O9jDUfzIUIoQw)
The structure of the folder is as follows:

```
(root)/Dataset/
        EV-Airborne/
            (sequence_name1).es
            (sequence_name2).es
            (sequence_name3).es
            .....
        EV-IMO/

        EV-IMO2/

        DistSurf/

        HKUST-EMS/

        EED/

```


# Run

### Setup the config file
The `config.yaml` file can be found in `./config` folder.

Choose the dataset name and the sequence name (same as the folder name in `./Dataset`)

Here are some examples of different parameters for different datasets:
```
data: EED
seq: what_is_background
....
....
```
```
data: EV-IMO
seq: box_seq01
....
....
```
```
data: EV-IMO2
seq: scene15_dyn_test_05_000000
....
....
```
```
data: HKUST-EMS
seq: cast
....
....
```
```
data: DistSurf
seq: cars
....
....
```
```
data: EV-Airborne
seq: recording_2023-04-26_14-53-22_cut
....
....
```

The `seq` name can be extracted from the `.es` file, for example, if the filename is:
`EED_what_is_background_events.es`

The `seq` name is `what_is_background`. It is always between the dataset name (e.g. `EED`) and `events`.

Other parameters are dependant on the dataset. Therefore, I am providing a simple `config.yaml` file to start, but feel free to experiement with different values.

## Execute motion segmentation

```
python main.py
```

The output from every layer of the network is saved in subfolders in `./output` in this format:

```
input_frames
RAFT_FlowImages_gap1
RAFT_Flows_gap1
coarse
bs
tt_adapt
rgb
motion_comp
motion_comp_large_delta
config_EV-Airborne_recording_2023-04-26_15-30-21_cut2.yaml
EV-Airborne_recording_2023-04-26_15-30-21_cut2_events_with_motion_inter.h5
motion_segmentation_network_EV-Airborne_recording_2023-04-26_15-30-21_cut2.gif
```

Description of the content of each folder:

- **input_frames**: The original time surface frames.
- **RAFT_FlowImages_gap1**: Optical flow images generated by RAFT.
- **RAFT_Flows_gap1**: The optical flow data in `.flo` format.
- **coarse:** The initial output from `TokenCut` which use the optical flow and the event time surface.
- **bs:** The output of the bilateral solver which is a binary mask for each frame. If `crf: true`, then a `crf` folder will be created.
- **tt_adapt**: The output from dynamic mask refinement that smooth and correct the mask from the `bs` or `crf`.
- **rgb**: Overlay between the timesurface frame and the `tt_adapt`.
- **motion_comp** The motion segmentation output
- **motion_comp_large_delta** The motion segmentation output but with a higher integration time `\delta t`.
- **config_EV-Airborne_recording_2023-04-26_15-30-21_cut2.yaml** The config file will be saved from `./config/config.yaml`.
- **EV-Airborne_recording_2023-04-26_15-30-21_cut2_events_with_motion_inter.h5**: The event with the continuous (i.e. motion) and discrete label (i.e. class id) it has the following structure: `'x','y','p','t','l','cl', 'vx', 'vy'`. `vx` and `vy` are the continuous motion labels and `cl` is the discrete label. Both are used to generate the motion segmentation output.
- **motion_segmentation_network_EV-Airborne_recording_2023-04-26_15-30-21_cut2.gif**: A video gif combining to show input and output of the motion segmentation. Plus there is gif file inside of each subfolder.

A faster implementation is also provided in `main_fast_single_object.py`, it only work if there is a single object moving in the field of view.
```
python main_fast_single_object.py
```


## Acknowledgement
This code is built on top of [TokenCut](https://github.com/YangtaoWANG95/TokenCut_video), [DINO](https://github.com/facebookresearch/dino), [RAFT](https://github.com/princeton-vl/RAFT), and [event_warping](https://github.com/neuromorphicsystems/event_warping) (our previous work). We would like to sincerely thanks those authors for their great works. 
